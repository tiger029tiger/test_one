{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 第5讲-分布式爬虫简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 基本概念介绍\n",
    "2. Python多线程\n",
    "3. Python多进程\n",
    "4. 集群化爬取介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 引子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 程序中存在3中类型的bug，你的bug，我的bug，和多线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 爬虫分布式一般是指多个分布式爬虫（worker） 从一个集中的任务队列（master）里拿任务，然后分布式的去爬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 爬虫属于IO密集型程序（网络IO和磁盘IO），这类程序的瓶颈大多在网络和磁盘读写的速度上\n",
    "- 多线程在一定程度上可加速爬虫的效率，但无法超过出口带宽，磁盘写的速度\n",
    "- Python的多线程，存在GIL的存在，存在编写难度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GIL全称Global Interpreter Lock，是一个防止多线程并发执行机器码的一个Mutex（互斥锁）\n",
    "- ref：http://cenalulu.github.io/python/gil-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.基本概念介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 并发 vs 并行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/para.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线程 vs 进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/mult1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/mult2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**初学者建议使用框架**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 队列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queue是Python标准库中的线程安全的队列（FIFO）实现,提供了一个适用于**多线程编程**的先进先出的数据结构，即队列，用来在生产者和消费者线程之间的信息传递"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基本FIFO队列\n",
    "\n",
    "class queue.Queue(maxsize=0)\n",
    "\n",
    "FIFO即First in First Out,先进先出。Queue提供了一个基本的FIFO容器，使用方法很简单,maxsize是个整数，指明了队列中能存放的数据个数的上限。一旦达到上限，插入会导致阻塞，直到队列中的数据被消费掉。如果maxsize小于或者等于0，队列大小没有限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "#queue.Queue类\n",
    "q = queue.Queue()\n",
    "\n",
    "for i in range(5):\n",
    "    q.put(i)\n",
    "\n",
    "while not q.empty():\n",
    "    print(q.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LIFO队列\n",
    "\n",
    "class Queue.LifoQueue(maxsize=0)\n",
    "\n",
    "LIFO即Last in First Out,后进先出。与栈的类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "#queue.LifoQueue类\n",
    "q = queue.LifoQueue()\n",
    "\n",
    "for i in range(5):\n",
    "    q.put(i)\n",
    "\n",
    "#empty 如果队列为空，返回True,反之返回False\n",
    "while not q.empty():\n",
    "    print(q.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优先级队列\n",
    "\n",
    "class Queue.PriorityQueue(maxsize=0)\n",
    "\n",
    "构造一个优先队列。maxsize用法同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Level 3 task\n",
      "Task: Level 10 task\n",
      "Task: Level 1 task\n",
      "For: Level 1 task\n",
      "For: Level 3 task\n",
      "For: Level 10 task\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "import threading\n",
    "\n",
    "class Task(object):\n",
    "    def __init__(self, priority, description):\n",
    "        self.priority = priority\n",
    "        self.description = description\n",
    "        print('Task:',description)\n",
    "        return\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.priority < other.priority\n",
    "    \n",
    "    #def __cmp__(self, other):\n",
    "     #   return cmp(self.priority, other.priority)\n",
    "\n",
    "q = queue.PriorityQueue()\n",
    "#put将item放入队列中。\n",
    "q.put(Task(3, 'Level 3 task'))\n",
    "q.put(Task(10, 'Level 10 task'))\n",
    "q.put(Task(1, 'Level 1 task'))\n",
    "\n",
    "def process_task(q):\n",
    "    while True:\n",
    "        #get从队列中移除并返回一个数据\n",
    "        next_task = q.get()\n",
    "        print('For:', next_task.description)\n",
    "        #队列的消费者线程调用,意味着入队的任务完成\n",
    "        q.task_done()\n",
    "\n",
    "workers = [threading.Thread(target=process_task, args=(q,)),\n",
    "        threading.Thread(target=process_task, args=(q,))\n",
    "        ]\n",
    "\n",
    "for w in workers:\n",
    "    w.setDaemon(True)\n",
    "    w.start()\n",
    "    \n",
    "#阻塞调用线程，直到队列中的所有任务（元素）处理完毕。\n",
    "q.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于threading.Thread()的使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第一种，PhantomJS是一个进程\n",
    "driver = webdriver.PhantomJS()   \n",
    "def test(url):\n",
    "    driver.get(url)\n",
    "\n",
    "url_list=[\"http://www.baidu.com\"]*10\n",
    "for url in url_list:\n",
    "     threading.Thread(target=test,args=(url,)).start() \n",
    "d.quit()\n",
    "\n",
    "#第二种，PhantomJS是每次开一个进程，10个进程\n",
    "def test(url):\n",
    "    driver = webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "    driver.quit()\n",
    "\n",
    "url_list=[\"http://www.baidu.com\"]*10\n",
    "for url in url_list:\n",
    "    threading.Thread(target=test,args=(url,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Python多线程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-0 with value 0\n",
      "Thread-1 with value 1Thread-2 with value 2Thread-3 with value 3\n",
      "\n",
      "Thread-4 with value 4Thread-5 with value 5Thread-6 with value 6Thread-7 with value 7Thread-8 with value 8\n",
      "Thread-9 with value 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#线程切换时乱掉的弊端的. 没有mutex\n",
    "import threading\n",
    "import time\n",
    "\n",
    "global_value = 0\n",
    "\n",
    "def run(threadName, lock):\n",
    "    global global_value\n",
    "    # 请求一个线程锁\n",
    "    lock.acquire()\n",
    "    local_copy = global_value\n",
    "    print(\"%s with value %s\" % (threadName, local_copy))\n",
    "    global_value = local_copy + 1\n",
    "    #释放一个线程锁\n",
    "    lock.release()\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "for i in range(10):\n",
    "    t = threading.Thread(target = run, args = (\"Thread-\" + str(i), lock))\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread: Thread-20 - 0\n",
      "Thread: Thread-20 - 1\n",
      "Thread: Thread-20 - 2\n",
      "Thread: Thread-20 - 3\n",
      "Thread: Thread-21 - 0\n",
      "Thread: Thread-21 - 1\n",
      "Thread: Thread-21 - 2\n",
      "Thread: Thread-21 - 3\n",
      "Thread: Thread-21 - 4\n",
      "Thread: Thread-21 - 5This program has finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "class MyThread(threading.Thread):\n",
    "\n",
    "    def __init__(self, count):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.total = count\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        for i in range(self.total):\n",
    "            time.sleep(1)\n",
    "            print(\"Thread: %s - %s\" % (self.name, i))\n",
    "\n",
    "t = MyThread(4)\n",
    "t2 = MyThread(6)\n",
    "\n",
    "t.start()\n",
    "t.join()\n",
    "t2.start()\n",
    "# 如果这里有文件操作？\n",
    "#t.join()\n",
    "t2.join()\n",
    "\n",
    "print(\"This program has finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多线程遍历网站列表-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.fudan.edu.cn: 200\n",
      "http://zimp.zju.edu.cn/: 200\n",
      "http://www.ruc.edu.cn/: 200\n",
      "http://www.pku.edu.cn/: 200\n",
      "http://www.douban.com: 200\n",
      "http://www.tsinghua.edu.cn: 200\n",
      "2.5551462173461914\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "import urllib\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "\n",
    "sites = [\n",
    "        \"http://www.fudan.edu.cn\",\n",
    "        \"http://www.douban.com\",\n",
    "       \"http://zimp.zju.edu.cn/\",\n",
    "       \"http://www.pku.edu.cn/\",\n",
    "    \"http://www.tsinghua.edu.cn\",\n",
    "    \"http://www.ruc.edu.cn/\"\n",
    "        ]\n",
    "\n",
    "def check_http_status(url):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "\n",
    "    request = urllib.request.Request(url = url, headers=headers)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    \n",
    "    return response.getcode()\n",
    "\n",
    "http_status = {}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for url in sites:\n",
    "    http_status[url] = check_http_status(url)\n",
    "\n",
    "    \n",
    "for url in http_status:\n",
    "    print(\"%s: %s\" % (url, http_status[url]))\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多线程遍历网站列表-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "sites = [\n",
    "        \"http://www.fudan.edu.cn\",\n",
    "        \"http://www.douban.com\",\n",
    "       \"http://zimp.zju.edu.cn/\",\n",
    "       \"http://www.pku.edu.cn/\",\n",
    "    \"http://www.tsinghua.edu.cn\",\n",
    "    \"http://www.ruc.edu.cn/\"\n",
    "        ]\n",
    "\n",
    "\n",
    "class HTTPStatusChecker(threading.Thread):\n",
    "    def __init__(self, url):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.url = url\n",
    "        self.status = None\n",
    "\n",
    "    def getURL(self):\n",
    "        return self.url\n",
    "\n",
    "    def getStatus(self):\n",
    "        return self.status\n",
    "\n",
    "    def run(self):\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "        request = urllib.request.Request(url = url, headers=headers)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        self.status = response.getcode()\n",
    "\n",
    "\n",
    "threads = []\n",
    "start = time.time()\n",
    "for url in sites:\n",
    "    t = HTTPStatusChecker(url)\n",
    "    t.start() #线程启动\n",
    "    threads.append(t) \n",
    "\n",
    "\n",
    "#主线程阻塞，等待其他完成\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "for  t in threads:\n",
    "    print(\"%s: %s\" % (t.url, t.status))\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线程安全的问题，用队列queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 线程间同步与互斥，线程间数据的共享（都涉及线程安全）\n",
    "- 同步、互斥机制：\n",
    "  - mutex\n",
    "  - condition\n",
    "  - event\n",
    "\n",
    "- 死锁and线程安全"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用队列（保证线程的安全）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009999275207519531Thread-35begin downloadhttp://www.fudan.edu.cn...Thread-36begin downloadhttp://www.pku.edu.cn...Thread-34begin downloadhttp://www.tsinghua.edu.cn...Thread-38begin downloadhttp://www.douban.com...Thread-37begin downloadhttp://zimp.zju.edu.cn...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thread-37download completed!\n",
      "Thread-37begin downloadhttp://www.ruc.edu.cn...\n",
      "Thread-34download completed!\n",
      "Thread-36download completed!\n",
      "Thread-35download completed!\n",
      "Thread-38download completed!\n",
      "Thread-37download completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import queue\n",
    "import threading\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import time\n",
    "\n",
    "class DownloadThread(threading.Thread):\n",
    "    def __init__(self, queue):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = queue\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            url = self.queue.get()\n",
    "            print(self.name + \"begin download\"+url+\"...\")\n",
    "            self.download_file(url)\n",
    "            self.queue.task_done()\n",
    "            print(self.name + \"download completed!\")\n",
    "    def download_file(self, url):\n",
    "        \n",
    "        \n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "        request = urllib.request.Request(url = url, headers=headers)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        \n",
    "        fname = os.path.basename(url) + \".html\"\n",
    "        with open(fname, \"wb\") as f:\n",
    "            while True:\n",
    "                chunk = response.read(1024)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f.write(chunk)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sites = [\"http://wiki.python.org/moin/WebProgramming\",\n",
    "            \"http://wiki.python.org/moin/Documentation\",\n",
    "            \"https://wiki.python.org/moin/WebFrameworks\",\n",
    "            \"https://wiki.python.org/moin/WebApplications\",\n",
    "            \"https://wiki.python.org/moin/BeginnersGuide\",\n",
    "            \"https://wiki.python.org/moin/BeginnersGuide/Overview\",\n",
    "            \"https://book.douban.com/subject/25862578\",\n",
    "            \"https://book.douban.com/subject/26698660\",\n",
    "            \"https://book.douban.com/subject/26957760\",\n",
    "            \"https://book.douban.com/subject/6082808\",\n",
    "            \"https://book.douban.com/subject/26878124\"\n",
    "            ]\n",
    "    \n",
    "    urls = [\n",
    "        \"http://www.fudan.edu.cn\",\n",
    "        \"http://www.douban.com\",\n",
    "       \"http://zimp.zju.edu.cn\",\n",
    "       \"http://www.pku.edu.cn\",\n",
    "    \"http://www.tsinghua.edu.cn\",\n",
    "    \"http://www.ruc.edu.cn\"\n",
    "        ]\n",
    "    \n",
    "    start = time.time()\n",
    "    queue = queue.Queue()\n",
    "\n",
    "    # 建立线程池，组合一个队列\n",
    "    for i in range(5):\n",
    "        t = DownloadThread(queue)  # 启动5个线程\n",
    "        # setDaemon用来设定线程的daemon属性，True表示主线程的退出可以不用等待子线程完成\n",
    "        # 默认为False，即所有非守护线程结束后主线程才结束\n",
    "        #thread不支持守护线程\n",
    "        t.setDaemon(True)\n",
    "        t.start()\n",
    "\n",
    "    for url in urls:\n",
    "        queue.put(url)\n",
    "        \n",
    "    #?queue.join()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线程安全的问题，用线程池模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pip install threadpool\n",
    "# conda无法安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threadpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将请求放入线程池开始下载\n",
      " http://wiki.python.org/moin/WebProgramming\n",
      "开始下载 http://wiki.python.org/moin/Documentation\n",
      "开始下载 http://zimp.zju.edu.cn\n",
      "开始下载 http://www.pku.edu.cn\n",
      "退出前销毁所有线程\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "import time\n",
    "import threadpool\n",
    "\n",
    "def download_file(url):\n",
    "    print(\"开始下载\", url)\n",
    "    \n",
    "    \n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "    request = urllib.request.Request(url = url, headers=headers)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    \n",
    "    fname = os.path.basename(url)+\".html\"\n",
    "    with open(fname, \"wb\") as f:\n",
    "        while True:\n",
    "            chunk = response.read(1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "\n",
    "urls = [\n",
    "        \"http://wiki.python.org/moin/WebProgramming\",\n",
    "        \"http://wiki.python.org/moin/Documentation\"\n",
    "        ]\n",
    "\n",
    "pool_size = 2\n",
    "pool = threadpool.ThreadPool(pool_size)\n",
    "\n",
    "# 创建工作请求\n",
    "requests = threadpool.makeRequests(download_file, urls)\n",
    "# 将工作请求放入队列\n",
    "[pool.putRequest(req) for req in requests]\n",
    "\n",
    "print(\"将请求放入线程池\")\n",
    "pool.putRequest(threadpool.WorkRequest(download_file, args=[\"http://zimp.zju.edu.cn\",]))\n",
    "pool.putRequest(threadpool.WorkRequest(download_file, args=[\"http://www.pku.edu.cn\",]))\n",
    "\n",
    "# 处理队列中新请求\n",
    "pool.poll()\n",
    "\n",
    "# 阻塞用于等待所有执行结果\n",
    "pool.wait()\n",
    "print(\"退出前销毁所有线程\")\n",
    "#告知pool_size大小的工作进程，在执行完当前任务退出\n",
    "pool.dismissWorkers(pool_size, do_join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬虫多线程优化案例（I）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "SO_URL = \"http://scifi.stackexchange.com\"\n",
    "QUESTION_LIST_URL = SO_URL + \"/questions\"\n",
    "MAX_PAGE_COUNT = 2\n",
    "\n",
    "global_results = []\n",
    "#初始页面，第一页\n",
    "initial_page = 1\n",
    "\n",
    "def get_author_name(body):\n",
    "    link_name = body.select(\".user-details a\")\n",
    "    if len(link_name) == 0:\n",
    "        text_name = body.select(\".user-details\")\n",
    "        return text_name[0].text if len(text_name) > 0 else 'N/A'\n",
    "    else:\n",
    "        return link_name[0].text\n",
    "\n",
    "def get_question_answers(body):\n",
    "    answers = body.select(\".answer\")\n",
    "    a_data = []\n",
    "    if len(answers) == 0:\n",
    "        return a_data\n",
    "\n",
    "    for a in answers:\n",
    "        data = {\n",
    "            'body': a.select(\".post-text\")[0].get_text(),\n",
    "            'author': get_author_name(a)\n",
    "        }\n",
    "        a_data.append(data)\n",
    "    return a_data\n",
    "\n",
    "def get_question_data ( url ): \n",
    "    print(\"Getting data from question page: %s \" % (url))\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Error while trying to scrape url: %s\" % (url))\n",
    "        return\n",
    "    body_soup = BeautifulSoup(resp.text, 'lxml')\n",
    "    # 将输出定义为JSON格式\n",
    "    q_data = {\n",
    "        'title': body_soup.select('#question-header .question-hyperlink')[0].text,\n",
    "        'body': body_soup.select('#question .post-text')[0].get_text(),\n",
    "        'author': get_author_name(body_soup.select(\".post-signature.owner\")[0]),\n",
    "        'answers': get_question_answers(body_soup)\n",
    "    }\n",
    "    return q_data\n",
    "\n",
    "\n",
    "def get_questions_page ( page_num, partial_results ):\n",
    "    print(\"=====================================================\")\n",
    "    print(\" Getting list of questions for page %s\" % (page_num))\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "    url = QUESTION_LIST_URL + \"?sort=newest&page=\" + str(page_num)\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'}\n",
    "    resp = \trequests.get(url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Error while trying to scrape url: %s\" % (url))\n",
    "        return\n",
    "    body = resp.text\n",
    "    main_soup = BeautifulSoup(body, 'lxml')\n",
    "\n",
    "    #获取每个提问的url\n",
    "    questions = main_soup.select('.question-summary .question-hyperlink')\n",
    "    urls = [ SO_URL + x['href'] for x in questions]\n",
    "    for url in urls:\n",
    "        q_data = get_question_data(url)\n",
    "        partial_results.append(q_data)\n",
    "    if page_num < MAX_PAGE_COUNT:\n",
    "        get_questions_page(page_num + 1, partial_results)\n",
    "\n",
    "\n",
    "get_questions_page(initial_page, global_results)\n",
    "with open('scrapping-results.json', 'w') as outfile:\n",
    "    json.dump(global_results, outfile, indent=4)\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "print('Results saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/io1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/io2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬虫多线程优化案例（II）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "===================================================== Getting list of questions for page 0.0\n",
      "\n",
      " Getting list of questions for page 1.0=====================================================\n",
      "\n",
      "=====================================================\n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157909/trying-to-identify-an-old-series-of-scifi-adventure-puzzle-picture-books \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157912/what-are-the-tv-movie-discworld-adaptations \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157922/how-many-pok%c3%a9mons-can-a-pok%c3%a9-ball-keep-at-a-time \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157900/apparent-earth-centricity-of-the-federation \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157909/trying-to-identify-an-old-series-of-scifi-adventure-puzzle-picture-books \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157922/how-many-pok%c3%a9mons-can-a-pok%c3%a9-ball-keep-at-a-time \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157912/what-are-the-tv-movie-discworld-adaptations \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157900/apparent-earth-centricity-of-the-federation \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157912/what-are-the-tv-movie-discworld-adaptations \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157898/can-the-weather-such-as-wind-have-any-effect-on-the-trajectory-of-spells-in-harr \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157898/can-the-weather-such-as-wind-have-any-effect-on-the-trajectory-of-spells-in-harr \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157909/trying-to-identify-an-old-series-of-scifi-adventure-puzzle-picture-books \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157909/trying-to-identify-an-old-series-of-scifi-adventure-puzzle-picture-books \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157893/are-pok%c3%a9mon-animals \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157900/apparent-earth-centricity-of-the-federation \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157889/why-would-the-changeling-take-garak-to-sisko \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157900/apparent-earth-centricity-of-the-federation \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157885/why-did-sonmi-451-suggest-this-to-the-archivist \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157898/can-the-weather-such-as-wind-have-any-effect-on-the-trajectory-of-spells-in-harr \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157883/young-woman-finds-herself-in-magical-world-then-begins-to-remember-living-or-vis \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157893/are-pok%c3%a9mon-animals \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157893/are-pok%c3%a9mon-animals \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157889/why-would-the-changeling-take-garak-to-sisko \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157889/why-would-the-changeling-take-garak-to-sisko \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157885/why-did-sonmi-451-suggest-this-to-the-archivist \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157898/can-the-weather-such-as-wind-have-any-effect-on-the-trajectory-of-spells-in-harr \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157882/how-did-bobbie-draper-get-her-power-armour-back \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157885/why-did-sonmi-451-suggest-this-to-the-archivist \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157880/in-dragon-ball-super-what-is-the-difference-between-destroying-and-erasing \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157883/young-woman-finds-herself-in-magical-world-then-begins-to-remember-living-or-vis \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157876/given-a-magical-world-why-is-the-quibbler-ridiculous \n",
      "Getting data from question page: http://scifi.stackexchange.com/questions/157883/young-woman-finds-herself-in-magical-world-then-begins-to-remember-living-or-vis \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "SO_URL = \"http://scifi.stackexchange.com\"\n",
    "QUESTION_LIST_URL = SO_URL + \"/questions\"\n",
    "MAX_PAGE_COUNT = 2\n",
    "\n",
    "\n",
    "class ThreadManager:\n",
    "    instance = None\n",
    "    final_results = []\n",
    "    threads_done = 0\n",
    "    totalConnections = 2\n",
    "    #并行线程的数量\n",
    "\n",
    "    @staticmethod\n",
    "    def notify_connection_end( partial_results ):\n",
    "        print(\"==== Thread is done! =====\")\n",
    "        ThreadManager.threads_done += 1\n",
    "        ThreadManager.final_results += partial_results\n",
    "        if ThreadManager.threads_done == ThreadManager.totalConnections:\n",
    "            print(\"==== Saving data to file! ====\")\n",
    "            with open('scrapping-results-optimized.json', 'w') as outfile:\n",
    "                json.dump(ThreadManager.final_results, outfile, indent=4)\n",
    "\n",
    "\n",
    "def get_author_name(body):\n",
    "    link_name = body.select(\".user-details a\")\n",
    "    if len(link_name) == 0:\n",
    "        text_name = body.select(\".user-details\")\n",
    "        return text_name[0].text if len(text_name) > 0 else 'N/A'\n",
    "    else:\n",
    "        return link_name[0].text\n",
    "\n",
    "def get_question_answers(body):\n",
    "    answers = body.select(\".answer\")\n",
    "    a_data = []\n",
    "    if len(answers) == 0:\n",
    "        return a_data\n",
    "\n",
    "    for a in answers:\n",
    "        data = {\n",
    "            'body': a.select(\".post-text\")[0].get_text(),\n",
    "            'author': get_author_name(a) \n",
    "        }\n",
    "        a_data.append(data)\n",
    "        return a_data\n",
    "\n",
    "\n",
    "\n",
    "def get_question_data ( url ):\n",
    "    print(\"Getting data from question page: %s \" % (url))\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'}\n",
    "    \n",
    "    resp = requests.get(url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Error while trying to scrape url: %s\" % (url))\n",
    "        return\n",
    "    \n",
    "    body_soup = BeautifulSoup(resp.text, 'lxml')\n",
    "    #转成JSON格式\n",
    "    q_data = {\n",
    "        'title': body_soup.select('#question-header .question-hyperlink')[0].text,\n",
    "        'body': body_soup.select('#question .post-text')[0].get_text(),\n",
    "        'author': get_author_name(body_soup.select(\".post-signature.owner\")[0]),\n",
    "        'answers': get_question_answers(body_soup)\n",
    "    }\n",
    "    return q_data\n",
    "\n",
    "\n",
    "def get_questions_page ( page_num, end_page, partial_results  ):\n",
    "    print(\"=====================================================\")\n",
    "    print(\" Getting list of questions for page %s\" % (page_num))\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "    url = QUESTION_LIST_URL + \"?sort=newest&page=\" + str(page_num)\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'}\n",
    "    \n",
    "    resp = \trequests.get(url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Error while trying to scrape url: %s\" % (url))\n",
    "    else:\n",
    "        body = resp.text\n",
    "        main_soup = BeautifulSoup(body, 'lxml')\n",
    "\n",
    "        #获取每个问题的url\n",
    "        questions = main_soup.select('.question-summary .question-hyperlink')\n",
    "        urls = [ SO_URL + x['href'] for x in questions]\n",
    "        for url in urls:\n",
    "            q_data = get_question_data(url)\n",
    "            partial_results.append(q_data)\n",
    "    if page_num + 1 < end_page:\n",
    "        get_questions_page(page_num + 1,  end_page, partial_results)\n",
    "    else:\n",
    "        ThreadManager.notify_connection_end(partial_results)\n",
    "\n",
    "pages_per_connection = MAX_PAGE_COUNT / ThreadManager.totalConnections\n",
    "\n",
    "\n",
    "for i in range(ThreadManager.totalConnections):\n",
    "    init_page = i * pages_per_connection\n",
    "    end_page = init_page + pages_per_connection\n",
    "    t = threading.Thread(target=get_questions_page, args=(init_page, end_page, [],  ),name='connection-%s' % (i))\n",
    "    t.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Python多进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多线程遇到的GIL问题，所以用多进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线程：\n",
    "1. 频繁IO操作\n",
    "2. 并行任务通过并发解决\n",
    "3. GUI开发\n",
    "\n",
    "不用线程：\n",
    "1. 频繁CPU操作\n",
    "2. 利用多核操作系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 多进程的优势：\n",
    "\n",
    "  - 可使用多核\n",
    "  - 进程使用独立内存空间，避免竞争问题\n",
    "  - 避开GIL限制\n",
    "  \n",
    "- 多进程劣势：\n",
    "\n",
    "  - 更多内存消耗\n",
    "  - 进程间数据共享困难\n",
    "  - 进程间通信处理比线程更困难"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- jupyter下不能跑\n",
    "- 可以建立.py源文件，在terminal里运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "1 简单进程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def run(pname):\n",
    "    print(pname)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    for i in range(10):\n",
    "        p = multiprocessing.Process(target=run, args=(\"Process-\" + str(i), ))\n",
    "        p.start()\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 进程通讯  queue， pipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 两个进程利用Queue进行通信\n",
    "\n",
    "from multiprocessing import Queue, Process\n",
    "import random\n",
    "\n",
    "def generate(q):\n",
    "    while True:\n",
    "        value = random.randrange(10)\n",
    "        q.put(value)\n",
    "        print(\"Value added to queue: %s\" % (value))\n",
    "\n",
    "def reader(q):\n",
    "    while True:\n",
    "        value = q.get()\n",
    "        print(\"Value from queue: %s\" % (value))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    queue = Queue()\n",
    "    p1 = Process(target=generate, args=(queue,))\n",
    "    p2 = Process(target=reader, args=(queue,))\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 两个进程利用Pipe进行通信\n",
    "\n",
    "from multiprocessing import Pipe, Process\n",
    "import random\n",
    "\n",
    "def generate(pipe):\n",
    "    while True:\n",
    "        value = random.randrange(10)\n",
    "        pipe.send(value)\n",
    "        print(\"Value sent: %s\" % (value))\n",
    "\n",
    "def reader(pipe):\n",
    "    f = open(\"output.txt\", \"w\")\n",
    "    while True:\n",
    "        value = pipe.recv()\n",
    "        f.write(str(value))\n",
    "        print(\"... ...\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_p, output_p = Pipe()\n",
    "    p1 = Process(target=generate, args=(input_p,))\n",
    "    p2 = Process(target=reader, args=(output_p,))\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用requests库，爬取漫画(单线程)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下载页面 http://xkcd.com\n",
      "正在下载漫画 http://imgs.xkcd.com/comics/survivorship_bias.png\n",
      "下载页面 http://xkcd.com/1826/\n",
      "正在下载漫画 http://imgs.xkcd.com/comics/birdwatching.png\n",
      "下载页面 http://xkcd.com/1825/\n",
      "正在下载漫画 http://imgs.xkcd.com/comics/7_eleven.png\n",
      "下载页面 http://xkcd.com/1824/\n",
      "正在下载漫画 http://imgs.xkcd.com/comics/identification_chart.png\n",
      "下载页面 http://xkcd.com/1823/\n",
      "正在下载漫画 http://imgs.xkcd.com/comics/hottest_editors.png\n",
      "下载页面 http://xkcd.com/1822/\n",
      "正在下载漫画 http://imgs.xkcd.com/comics/existential_bug_reports.png\n",
      "下载页面 http://xkcd.com/1821/\n",
      "正在下载漫画 http://imgs.xkcd.com/comics/incinerator.png\n",
      "下载页面 http://xkcd.com/1820/\n",
      "正在下载漫画 http://imgs.xkcd.com/comics/security_advice.png\n",
      "下载页面 http://xkcd.com/1819/\n",
      "正在下载漫画 http://imgs.xkcd.com/comics/sweet_16.png\n",
      "下载页面 http://xkcd.com/1818/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-09271f206f0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 下载页面\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'下载页面 %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m# 如果下载发生问题，抛出异常，同时终止程序\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    486\u001b[0m         }\n\u001b[0;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             )\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    421\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m                 )\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1199\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\requests\\packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Unexpected EOF'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\data\\Anaconda3\\envs\\py3_scrapy\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1332\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### 爬取漫画的案例，使用requests库\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://xkcd.com'\n",
    "os.makedirs('./dataTm/xkcd', exist_ok = True)\n",
    "while not url.endswith('#'):\n",
    "    # 下载页面\n",
    "    print('下载页面 %s' % url)\n",
    "    res = requests.get(url)\n",
    "    # 如果下载发生问题，抛出异常，同时终止程序\n",
    "    res.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    # 寻找漫画的地址\n",
    "    comicElement = soup.select('#comic img')\n",
    "    #print(comicElement)\n",
    "    if comicElement == []:\n",
    "        print('无法找到图片.')\n",
    "    else:\n",
    "        comicURL = 'http:' + comicElement[0].get('src')\n",
    "        # 下载漫画.\n",
    "        print('正在下载漫画 %s' % (comicURL))\n",
    "        res = requests.get(comicURL)\n",
    "        res.raise_for_status()\n",
    "\n",
    "    # 下载完成之后，将图片保存到xkcd文件夹\n",
    "    imageFile = open(os.path.join('./dataTm/xkcd', os.path.basename(comicURL)), 'wb')\n",
    "    # 这里与之前的保存文件的方式不同，利用的是迭代写入，提高性能\n",
    "    for chunk in res.iter_content(10000):\n",
    "        imageFile.write(chunk)\n",
    "    imageFile.close()\n",
    "\n",
    "    # 下载完之后，找前一副图片的地址\n",
    "    PrevLink = soup.select('a[rel=\"prev\"]')[0]\n",
    "    #print(soup.select('a[rel=\"prev\"]'))\n",
    "    url = 'http://xkcd.com' + PrevLink.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tips：\n",
    "\n",
    "以上为强行中断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用requests库，爬取漫画(多线程)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下载页面 http://xkcd.com/1\n",
      "下载页面 http://xkcd.com/11\n",
      "下载页面 http://xkcd.com/21\n",
      "下载页面 http://xkcd.com/31\n",
      "下载页面 http://xkcd.com/41\n",
      "下载页面 http://xkcd.com/51\n",
      "下载页面 http://xkcd.com/71\n",
      "下载页面 http://xkcd.com/61下载页面 http://xkcd.com/81\n",
      "\n",
      "下载页面 http://xkcd.com/91\n",
      "下载图片 http://imgs.xkcd.com/comics/barrel_cropped_(1).jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/barrel_mommies.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/kepler.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/pwned.png\n",
      "下载图片 http://imgs.xkcd.com/comics/attention_shopper.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/in_the_trees.jpg\n",
      "下载页面 http://xkcd.com/2\n",
      "下载图片 http://imgs.xkcd.com/comics/malaria.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/staceys_dad.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/barrel_part_5.jpg\n",
      "下载页面 http://xkcd.com/12\n",
      "下载页面 http://xkcd.com/92\n",
      "下载图片 http://imgs.xkcd.com/comics/unspeakable_pun.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/tree_cropped_(1).jpg\n",
      "下载页面 http://xkcd.com/22\n",
      "下载页面 http://xkcd.com/52\n",
      "下载页面 http://xkcd.com/72\n",
      "下载页面 http://xkcd.com/82\n",
      "下载图片 http://imgs.xkcd.com/comics/poisson.jpg\n",
      "下载页面 http://xkcd.com/62\n",
      "下载页面 http://xkcd.com/32\n",
      "下载图片 http://imgs.xkcd.com/comics/sunrise.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/barrel_whirlpool.jpg\n",
      "下载页面 http://xkcd.com/3\n",
      "下载图片 http://imgs.xkcd.com/comics/karnaugh.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/secret_worlds.jpg\n",
      "下载页面 http://xkcd.com/42\n",
      "下载图片 http://imgs.xkcd.com/comics/frame.jpg\n",
      "下载页面 http://xkcd.com/13\n",
      "下载页面 http://xkcd.com/23\n",
      "下载页面 http://xkcd.com/93\n",
      "下载图片 http://imgs.xkcd.com/comics/classhole.jpg\n",
      "下载页面 http://xkcd.com/63\n",
      "下载页面 http://xkcd.com/83\n",
      "下载页面 http://xkcd.com/53\n",
      "下载图片 http://imgs.xkcd.com/comics/t-shirts.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/island_color.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/jeremy_irons.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/canyon_small.jpg\n",
      "下载页面 http://xkcd.com/94\n",
      "下载图片 http://imgs.xkcd.com/comics/hobby.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/pillar.jpg\n",
      "下载页面 http://xkcd.com/73\n",
      "下载图片 http://imgs.xkcd.com/comics/geico.jpg\n",
      "下载页面 http://xkcd.com/24\n",
      "下载页面 http://xkcd.com/4\n",
      "下载图片 http://imgs.xkcd.com/comics/valentine.jpg\n",
      "下载页面 http://xkcd.com/14\n",
      "下载图片 http://imgs.xkcd.com/comics/katamari.jpg\n",
      "下载页面 http://xkcd.com/33\n",
      "下载图片 http://imgs.xkcd.com/comics/profile_flowchart.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/godel_escher_kurthalsey.jpg\n",
      "下载页面 http://xkcd.com/43\n",
      "下载图片 http://imgs.xkcd.com/comics/landscape_cropped_(1).jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/self-reference.jpg\n",
      "下载页面 http://xkcd.com/84\n",
      "下载页面 http://xkcd.com/95\n",
      "下载页面 http://xkcd.com/54\n",
      "下载页面 http://xkcd.com/64\n",
      "下载图片 http://imgs.xkcd.com/comics/copyright.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/red_spiders_2.jpg\n",
      "下载页面 http://xkcd.com/34\n",
      "下载图片 http://imgs.xkcd.com/comics/the_sierpinski_penis_game.jpg\n",
      "下载页面 http://xkcd.com/5\n",
      "下载图片 http://imgs.xkcd.com/comics/national_language.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/solar_plexus.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/science.jpg\n",
      "下载页面 http://xkcd.com/44\n",
      "下载页面 http://xkcd.com/15\n",
      "下载图片 http://imgs.xkcd.com/comics/flowers.jpg\n",
      "下载页面 http://xkcd.com/65\n",
      "下载页面 http://xkcd.com/96\n",
      "下载图片 http://imgs.xkcd.com/comics/zeppelin.jpg\n",
      "下载页面 http://xkcd.com/85\n",
      "下载页面 http://xkcd.com/55\n",
      "下载页面 http://xkcd.com/35\n",
      "下载页面 http://xkcd.com/74\n",
      "下载图片 http://imgs.xkcd.com/comics/mail.png\n",
      "下载图片 http://imgs.xkcd.com/comics/banter.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/useless.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/paths.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/love.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/just_alerting_you.jpg\n",
      "下载页面 http://xkcd.com/97\n",
      "下载图片 http://imgs.xkcd.com/comics/sheep.jpg\n",
      "下载页面 http://xkcd.com/45\n",
      "下载页面 http://xkcd.com/86\n",
      "下载图片 http://imgs.xkcd.com/comics/blownapart_color.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/su_doku.jpg\n",
      "下载页面 http://xkcd.com/16\n",
      "下载页面 http://xkcd.com/56\n",
      "下载图片 http://imgs.xkcd.com/comics/a_simple_plan.jpg\n",
      "下载页面 http://xkcd.com/66\n",
      "下载图片 http://imgs.xkcd.com/comics/sony_microsoft_mpaa_riaa_apple.jpg\n",
      "下载页面 http://xkcd.com/75\n",
      "下载页面 http://xkcd.com/36\n",
      "下载图片 http://imgs.xkcd.com/comics/monty_python.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/the_cure.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/schrodinger.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/abusive_astronomy.jpg\n",
      "下载页面 http://xkcd.com/98\n",
      "下载页面 http://xkcd.com/46\n",
      "下载图片 http://imgs.xkcd.com/comics/curse_levels.jpg\n",
      "下载页面 http://xkcd.com/67\n",
      "下载页面 http://xkcd.com/25\n",
      "下载页面 http://xkcd.com/17\n",
      "下载页面 http://xkcd.com/76\n",
      "下载页面 http://xkcd.com/57\n",
      "下载图片 http://imgs.xkcd.com/comics/secrets.jpg\n",
      "下载页面 http://xkcd.com/6\n",
      "下载图片 http://imgs.xkcd.com/comics/scientists.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/wait_for_me.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/familiar.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/what_if.jpg\n",
      "下载页面 http://xkcd.com/47\n",
      "下载图片 http://imgs.xkcd.com/comics/fall_apart.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/irony_color.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/nerd_girls.jpg\n",
      "下载页面 http://xkcd.com/99\n",
      "下载图片 http://imgs.xkcd.com/comics/barrel_part_4.jpg\n",
      "下载页面 http://xkcd.com/77\n",
      "下载页面 http://xkcd.com/58\n",
      "下载页面 http://xkcd.com/37\n",
      "下载页面 http://xkcd.com/7\n",
      "下载图片 http://imgs.xkcd.com/comics/binary_heart.jpg\n",
      "下载页面 http://xkcd.com/18\n",
      "下载页面 http://xkcd.com/87\n",
      "下载页面 http://xkcd.com/26\n",
      "下载图片 http://imgs.xkcd.com/comics/counter-red-spiders.jpg\n",
      "下载页面 http://xkcd.com/68\n",
      "下载图片 http://imgs.xkcd.com/comics/bored_with_the_internet.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/girl_sleeping_noline_(1).jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/why_do_you_love_me.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/velociraptors.jpg\n",
      "下载页面 http://xkcd.com/48\n",
      "下载图片 http://imgs.xkcd.com/comics/hyphen.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/snapple.jpg\n",
      "下载页面 http://xkcd.com/38\n",
      "下载图片 http://imgs.xkcd.com/comics/five_thirty.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/found.jpg\n",
      "下载页面 http://xkcd.com/19\n",
      "下载页面 http://xkcd.com/49\n",
      "下载页面 http://xkcd.com/8\n",
      "下载页面 http://xkcd.com/69\n",
      "下载图片 http://imgs.xkcd.com/comics/george_clinton.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/fourier.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/red_spiders_small.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/pillow_talk.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/apple_jacks.jpg\n",
      "下载页面 http://xkcd.com/78\n",
      "下载页面 http://xkcd.com/9\n",
      "下载图片 http://imgs.xkcd.com/comics/want.jpg\n",
      "下载页面 http://xkcd.com/88\n",
      "下载页面 http://xkcd.com/27\n",
      "下载页面 http://xkcd.com/39\n",
      "下载图片 http://imgs.xkcd.com/comics/garfield.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/escher_wristband.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/meat_cereals.jpg\n",
      "下载页面 http://xkcd.com/89\n",
      "下载页面 http://xkcd.com/79\n",
      "下载图片 http://imgs.xkcd.com/comics/bowl.jpg\n",
      "下载页面 http://xkcd.com/28\n",
      "下载图片 http://imgs.xkcd.com/comics/gravitational_mass.jpg\n",
      "下载页面 http://xkcd.com/59\n",
      "下载图片 http://imgs.xkcd.com/comics/elefino.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/iambic_pentameter.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/graduation.jpg\n",
      "下载页面 http://xkcd.com/29\n",
      "下载图片 http://imgs.xkcd.com/comics/hitler.jpg\n",
      "下载图片 http://imgs.xkcd.com/comics/firefly.jpg\n",
      "完成\n"
     ]
    }
   ],
   "source": [
    "### 爬取漫画的案例，使用requests库\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "\n",
    "os.makedirs('xkcd', exist_ok = True)\n",
    "\n",
    "def downloadXkcd(startComic, endComic):\n",
    "    for urlNumber in range(startComic, endComic):\n",
    "        # 下载页面\n",
    "        print('下载页面 http://xkcd.com/%s' % (urlNumber))\n",
    "        res = requests.get('http://xkcd.com/%s' % (urlNumber))\n",
    "        res.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "        #确认漫画的页面url\n",
    "        comicElem = soup.select('#comic img')\n",
    "        if comicElem == []:\n",
    "            print('找不到该页面')\n",
    "        else:\n",
    "            comicUrl = comicElem[0].get('src')\n",
    "            comicUrl = 'http:'+comicUrl\n",
    "            # 下载图片\n",
    "            print('下载图片 %s' % (comicUrl))\n",
    "            res = requests.get(comicUrl)\n",
    "            res.raise_for_status()\n",
    "\n",
    "            #保存图片\n",
    "            imageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)), 'wb')\n",
    "            for chunk in res.iter_content(100000):\n",
    "                imageFile.write(chunk)\n",
    "            imageFile.close()\n",
    "\n",
    "#建立和开启线程\n",
    "downloadThreads = []\n",
    "#一个线程对象的列表\n",
    "for i in range(1, 100, 10):\n",
    "    downloadThread = threading.Thread(target = downloadXkcd, args=(i, i+9))\n",
    "    downloadThreads.append(downloadThread)\n",
    "    downloadThread.start()\n",
    "\n",
    "\n",
    "\n",
    "#等待各个线程结束\n",
    "for downloadThread in downloadThreads:\n",
    "    downloadThread.join()\n",
    "print('完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 关于多线程的并发问题的notes：\n",
    "\n",
    "- 创建多线程，同时运行，容易\n",
    "- 但由于线程同时读写，容易互相干扰，导致并发问题，且难以调试\n",
    "- 避免让多个线程读取或写入相同的变量，即，当创建一个新的Thread对象时，确保其目标函数只使用该函数中的局部变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.集群化爬取介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一台主机存放队列，其它机器负责去抓\n",
    "- 所有集群内机器能够充分有效利用队列进行抓取\n",
    "- MongoDB，redis作为队列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "基本的爬虫：维护一个队列\n",
    "\n",
    "1. 设置一个队列（可以用queue，可以用数据库比如mongdo或者redis）\n",
    "2. 把初始页面放入队列\n",
    "3. 从队列中提取url，进行爬取\n",
    "4. 从该url下所有的页面url，放入队列\n",
    "5. 从队列中抽取url，继续爬取\n",
    "6. 重复上述过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "集群化的方式：\n",
    "1. 一台机器维护队列\n",
    "2. 其余机器通过网络通信，从该台机器上取得url，进行爬取\n",
    "\n",
    "- 重复上述过程\n",
    "- 机器越多，越快\n",
    "- 争论：使用多机集群的目的，是为了速度还是防止反爬虫？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "集群化的写法：\n",
    "\n",
    "1. 在master机器上设置一个队列（可以用queue，可以用数据库比如mongdo或者redis）\n",
    "2. slaver机器（分布式集群）通过网络通信，从master机器的队列中提取url，进行爬取\n",
    "3. 从该url下所有的页面url，放入队列\n",
    "4. 从队列中抽取url，继续爬取\n",
    "5. 重复上述过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 涉及的问题较多，推荐使用成熟的框架和库\n",
    "\n",
    "比如：https://scrapy-redis.readthedocs.io/en/stable/\n",
    "见scrapy的章节介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
