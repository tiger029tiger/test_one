{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 第6讲-网络爬虫框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Python网络爬虫框架介绍\n",
    "2. Scrapy基本使用（shell工具）\n",
    "3. Scrapy模板框架\n",
    "4. Scrapy进阶使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Python网络爬虫框架介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 问题：\n",
    "- 随着数据的增多，网站类型的多样，爬取的难度增大\n",
    "  - 1.对技术难度的增加\n",
    "    - 多线程，多进程，分布式\n",
    "  - 2.对维护难度的增加\n",
    "  - 3.对合作使用难度的增加\n",
    "\n",
    "\n",
    "- 前提：爬虫的流程，虽然在细节上千变万化，但是基础的思路是一致的，存在抽象的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/pyframe1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/pyframe2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 需要作出的选择：\n",
    "  - 1.自己开发爬虫框架\n",
    "  - 2.使用其它优秀的框架\n",
    "\n",
    "- 对于1：\n",
    "  - 技术要求高\n",
    "  - 涉及的技术广度大，且反爬手段日新月异，需要对框架进行各种适应性调整\n",
    "- 优点：控制度高，技术磨炼\n",
    "- 缺点：打磨需要时间，新项目快速，健壮的需求\n",
    "\n",
    "- 对于2：\n",
    "  - 技术要求相对较低\n",
    "- 优点：快速上手，项目使用相对方便，便于团队合作\n",
    "- 缺点：使用场景被限制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结论：\n",
    "\n",
    "- 选择一款:\n",
    "  - 1. 便于使用\n",
    "  - 2. 文档齐全\n",
    "  - 3. 稳定性高\n",
    "  - 4. 使用相对广泛（）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PySpider\n",
    "2. Scrapy \n",
    "3. PSpider\n",
    "\n",
    "\n",
    "- PySpider相对Scrapy简单\n",
    "- 操作更加简便\n",
    "- 开发爬虫速度快\n",
    "- 拥有 web 界面\n",
    "- 在动态网页方面，集成phantomjs\n",
    "\n",
    "\n",
    "- Scrapy相对复杂（只是相对，比自己写简单）\n",
    "- 更加底层，自定义程度高\n",
    "- 支持多线程，分布式,使用中对多线程编程和分布式的理解会提高\n",
    "\n",
    "\n",
    "- https://scrapy.org/\n",
    "- http://docs.pyspider.org/en/latest/\n",
    "- http://www.pyspider.cn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PySpider：一个国人编写的强大的网络爬虫系统并带有强大的WebUI。采用Python语言编写，分布式架构，支持多种数据库后端，强大的WebUI支持脚本编辑器，任务监视器，项目管理器以及结果查看器。在线示例：http://demo.pyspider.org/\n",
    "\n",
    "\n",
    "> pyspider部分特性\n",
    "\n",
    "> python 脚本控制，可以用任何你喜欢的html解析包（内置 pyquery）\n",
    "\n",
    "> WEB 界面编写调试脚本，起停脚本，监控执行状态，查看活动历史，获取结果产出，脚本可具有去重调度，队列，抓取，异常处理，监控等功能\n",
    "\n",
    "> 数据存储支持MySQL, MongoDB, Redis, SQLite, Elasticsearch; PostgreSQL 及 SQLAlchemy\n",
    "\n",
    "> 队列服务支持RabbitMQ, Beanstalk, Redis 和 Kombu\n",
    "\n",
    "> 支持抓取 JavaScript 的页面\n",
    "\n",
    "> 组件可替换，支持单机/分布式部署，支持 Docker 部署\n",
    "\n",
    "> 强大的调度控制，支持超时重爬及优先级设置\n",
    "\n",
    "> 支持python2&3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 安装\n",
    "- pip install pyspider\n",
    "- 安装成功后运行\n",
    "- pyspider\n",
    "- 打开http://localhost:5000/ 访问控制台"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。 \n",
    "\n",
    "> Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。\n",
    "\n",
    "> Scrapy吸引人的地方在于我们可以根据需求方便的修改框架。\n",
    "\n",
    "> 提供了多种类型爬虫的基类， 如BaseSpider、sitemap爬虫等\n",
    "\n",
    "> 还有对web2.0爬虫的支持。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/pyframe2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy主要包括了以下组件：\n",
    "\n",
    "**Scrapy引擎**: 用来处理整个系统的数据流处理, 触发事务\n",
    "\n",
    "**调度器(Scheduler)**: 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回。可以想像成一个URL的优先队列, 由它来决定下一个要抓取的网址是什么, **同时去除重复的网址**\n",
    "\n",
    "**下载器(Downloader)**: 用于下载网页内容, 并将网页内容返回给蜘蛛\n",
    "\n",
    "twisted异步网络框架（https://twistedmatrix.com/trac/）\n",
    "\n",
    "（twisted是一个用python语言写的事件驱动的网络框架，支持多种协议，包括UDP,TCP,TLS和其他应用层协议，比如HTTP，SMTP，NNTM，IRC，XMPP/Jabber。）\n",
    "\n",
    "**爬虫(Spiders)**: 爬虫用于从特定的网页中提取自己需要的信息, 即Item（可以定义），用户也可以从中提取出链接,让Scrapy继续抓取下一个页面\n",
    "\n",
    "**项目管道(Pipeline)**: 负责处理爬虫从网页中抽取的item，主要的功能是持久化（数据库）、验证有效性、清除不需要的信息等。当页面被爬虫解析后，将被发送到项目管道，并经过特定流程处理数据。\n",
    "\n",
    "**各类中间件**：\n",
    "\n",
    "**下载器中间件(Downloader Middlewares)**: 位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。\n",
    "\n",
    "**爬虫中间件(Spider Middlewares)**: 位于Scrapy引擎和爬虫之间的框架，主要工作是处理爬虫的响应输入和请求输出。\n",
    "\n",
    "**调度中间件(Scheduler Middewares)**: 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。\n",
    "\n",
    "    \n",
    "> Scrapy运行流程大概如下：\n",
    "\n",
    "> 引擎从调度器中取出一个链接(URL)用于接下来的抓取\n",
    "\n",
    "> 引擎把URL封装成一个请求(Request)传给下载器，下载器把资源下载下来，并封装成响应包(Response)\n",
    "\n",
    "> 然后，爬虫解析Response,若是解析是Item,则交给实体管道进行进一步的处理；若是解析出的是URL,则把URL交给Scheduler等待抓取\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简单概括：\n",
    "\n",
    "1. 各部件设置setting\n",
    "2. 定义item\n",
    "3. 开始爬取（spider）\n",
    "4. \n",
    "    \n",
    "    - 4.1 放入流水线pipeline进行处理，入库等操作\n",
    "    - 4.2 或者继续抓取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它框架：\n",
    "\n",
    "https://github.com/xianhu/PSpider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Scrapy基本使用（shell工具）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy的安装：\n",
    "\n",
    "利用conda进行安装\n",
    "\n",
    "- 1.创建虚拟环境\n",
    "\n",
    "conda create -n scrapy_demo python=3.5   # 有些版本会有twisted的库的问题\n",
    "\n",
    "进入环境\n",
    "\n",
    "activate scrapy_demo\n",
    "\n",
    "- 2.conda install lxml   \n",
    "\n",
    "#依赖\n",
    "\n",
    "- 3.conda install scrapy\n",
    "\n",
    "结束\n",
    "\n",
    "scrapy version   # 测试版本情况，是否安装完整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note:为了后面的测试方便，我们可以安装ipython或者jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 先测试下：\n",
    "\n",
    "scrapy -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/scrapyh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrapy shell url   # 用来对目标网站进行探索\n",
    "scrapy shell url|file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrapy shell www.baidu.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跳转到ipython命令行界面，网页已被保存在response对象里，可以查看response\n",
    "\n",
    "> response\n",
    "\n",
    "输出状态码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrapy shell www.douban.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrapy shell -s USER_AGENT=\"Mozilla/5.0\"  url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用fetch()获取网页\n",
    "\n",
    "fetch('https://www.baidu.com')\n",
    "\n",
    "\n",
    "在交互的模式下，重新对一个url发送请求，自动更新到request和response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在scrapy shell中键入\n",
    "\n",
    "view(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用xpath匹配网页元素\n",
    "\n",
    "response.xpath('//*[@id=\"lg\"]').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分层调试\n",
    "\n",
    "a = response.xpath('//*[@id=\"lg\"]')\n",
    "\n",
    "a.xpath('./img').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response.body[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xpath() 和 css()\n",
    "response.css('#u1').xpath('//a').re('www.(.*?).com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://doc.scrapy.org/en/latest/topics/shell.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Scrapy模板框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单案例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取Stack Overflow的高分问题信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立python文件，so_spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class StackOverflowSpider(scrapy.Spider):\n",
    "    name = 'stackoverflow'\n",
    "    start_urls = ['http://stackoverflow.com/questions?sort=votes']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for href in response.css('.question-summary h3 a::attr(href)'):\n",
    "            full_url = response.urljoin(href.extract())\n",
    "            yield scrapy.Request(full_url, callback=self.parse_question)\n",
    "\n",
    "    def parse_question(self, response):\n",
    "        yield {\n",
    "            'title': response.css('h1 a::text').extract()[0],\n",
    "            'votes': response.css('.question .vote-count-post::text').extract()[0],\n",
    "            'body': response.css('.question .post-text').extract()[0],\n",
    "            'tags': response.css('.question .post-tag::text').extract(),\n",
    "            'link': response.url,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 命令行下操作，保存至json文件\n",
    "scrapy runspider so_spider.py -o so_question.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立项目模板：（打印，不做其它操作）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "在命令行下建立项目：\n",
    "\n",
    "scrapy startproject douban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了开发和测试的方便，引入IDE，pycharm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤：\n",
    "\n",
    "1. 使用pycharm打开项目\n",
    "2. 选择项目名目录\n",
    "3. 选择合适的解释器（env/douban/python.exe）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看项目结构："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dataTm/work_pic/proj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- items.py:\n",
    "\n",
    "自定义item类，爬虫爬到数据后，传入管道文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pipelines  管道文件，对传入的item数据进行清洗和入库\n",
    "\n",
    "\n",
    ">当一个item被蜘蛛爬取到之后会被发送给Item Pipeline，然后多个组件按照顺序处理这个item。 每个Item Pipeline组件其实就是一个实现了一个简单方法的Python类。他们接受一个item并在上面执行逻辑，还能决定这个item到底是否还要继续往下传输，如果不要了就直接丢弃。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Item Pipeline的常用场景：\n",
    "\n",
    "  - 清理HTML数据\n",
    "  - 验证被抓取的数据(检查item是否包含某些字段)\n",
    "  - 重复性检查(然后丢弃)\n",
    "  - 将抓取的数据存储到数据库中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- settings\n",
    "\n",
    "  - 设置文件\n",
    "  - 下载延时\n",
    "  - 项目管道文件中的类的启用和\n",
    "  - 中间件的启用和顺序\n",
    "  - robot的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spider目录\n",
    "\n",
    "定义爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- middleware \n",
    "\n",
    "定义中间件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doubantop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class DoubantopSpider(scrapy.Spider):\n",
    "    name = \"doubantop\"\n",
    "    start_urls = ['https://movie.douban.com/top250']\n",
    "\n",
    "    def parse(self, response):\n",
    "        print(response)\n",
    "        name_list = response.xpath(\"//div[@class=\"pic\"]/a/img/@alt\").extract()\n",
    "        rank_list = response.xpath('//div[@class=\"rank\"]//h1/text()').extract()\n",
    "        for name, rank in zip(name_list, rank_list):\n",
    "            print(name,\":\", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# //div[@class=\"pic\"]/a/img/@alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.建立item---->爬虫---->pipeline模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立数据库(自带关系数据库sqlite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> import sqlite3\n",
    "> douban(连接) = sqlite3.connect('douban(数据库).sqlite')\n",
    "> sql_command = 'create table douban(表名称) (name varchar(256), rank varchar(256))'\n",
    "> douban.execute(sql_command)\n",
    "> exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  A1.建立item---->爬虫---->pipeline模型（并不入库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1. 先修改settings：\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "    'douban.pipelines.DoubanPipeline': 300,\n",
    "}\n",
    "\n",
    "来自pipeline的定义,300约定顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2. 修改pipelines：\n",
    "\n",
    "\n",
    "class DoubanPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        print(spider.name)\n",
    "        return item\n",
    "\n",
    "pipeline做一件事情：item处理，入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3. 修改items：\n",
    "\n",
    "\n",
    "class DoubanItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    rank = scrapy.Field()\n",
    "    \n",
    "定义要爬取的item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "4. 修改spider\n",
    "\n",
    "import scrapy\n",
    "from douban.items import DoubanItem\n",
    "\n",
    "class DoubantopSpider(scrapy.Spider):\n",
    "    name = \"doubantop\"\n",
    "    start_urls = ['https://movie.douban.com/top250']\n",
    "\n",
    "    def parse(self, response):\n",
    "        print(response)\n",
    "\n",
    "        doubantop = DoubanItem()\n",
    "        name_list = response.xpath(\"//div[@class='name']/h1/text()\").extract()\n",
    "        rank_list = response.xpath('//div[@class=\"rank\"]//h1/text()').extract()\n",
    "        for name, rank in zip(name_list, rank_list):\n",
    "            doubantop['name'] = name\n",
    "            doubantop['rank'] = rank\n",
    "            yield doubantop\n",
    "\n",
    "不print，只是将爬虫爬下来的内容，放入pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  A2.建立item---->爬虫---->pipeline模型（入库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1. 只需要调整上述模型的一个部分：\n",
    "\n",
    "pipelines\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "import sqlite3\n",
    "\n",
    "class DoubanPipeline(object):\n",
    "\n",
    "    #This method is called when the spider is opened.\n",
    "    def open_spider(self, spider):\n",
    "        self.con = sqlite3.connect('douban.sqlite')\n",
    "        self.cu = self.con.cursor()\n",
    "\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        print(spider.name)\n",
    "        sql_insert = \"insert into douban (name, rank) values ('{}', '{}')\".format(item['name'], item['rank'])\n",
    "        self.cu.execute(sql_insert)\n",
    "        self.con.commit()\n",
    "        return item\n",
    "    \n",
    "    #This method is called when the spider is closed.\n",
    "    def spider_close(self, spider):\n",
    "        self.con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Scrapy进阶使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 案例入库MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> CREATE DATABASE douban；\n",
    "> CREATE TABLE douban (\n",
    "  rank VARCHAR(128),\n",
    "  title VARCHAR(128),\n",
    "  link varchar(128)\n",
    ");\n",
    ">INSERT INTO douban (rank, title, link)\n",
    "VALUES ('x', 'x', 'x');\n",
    "> exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setting的设置：\n",
    "    \n",
    "    \n",
    "BOT_NAME = 'douban'\n",
    "\n",
    "SPIDER_MODULES = ['douban.spiders']\n",
    "NEWSPIDER_MODULE = 'douban.spiders'\n",
    "\n",
    "# 记得shell操作的时候douban需要user agent\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'\n",
    "# MySQL 的配置\n",
    "MYSQL_HOST = 'localhost'\n",
    "MYSQL_DBNAME = 'douban'\n",
    "MYSQL_USER = 'root'\n",
    "MYSQL_PASSWD = '123456'\n",
    "\n",
    "\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "    #'douban.pipelines.JSONPipeline': 300,\n",
    "    'douban.pipelines.MySQLPipeline': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item的定义:\n",
    "    \n",
    "    \n",
    "import scrapy\n",
    "\n",
    "\n",
    "class DoubanItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    rank = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spider的定义:\n",
    "doubantop.py\n",
    "    \n",
    "import scrapy\n",
    "from douban.items import DoubanItem\n",
    "\n",
    "class DoubanSpider(scrapy.Spider):\n",
    "    name = 'doubantop'\n",
    "    allowed_domains = [\"douban.com\"]\n",
    "    start_urls = [\n",
    "        \"http://movie.douban.com/top250/\"\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        items = []\n",
    "        for info in response.xpath('//div[@class=\"item\"]'):\n",
    "            # 每一条电影总体信息\n",
    "            item = DoubanItem()\n",
    "            item['rank'] = info.xpath('div[@class=\"pic\"]/em/text()').extract()\n",
    "            item['title'] = info.xpath('div[@class=\"pic\"]/a/img/@alt').extract()\n",
    "            item['link'] = info.xpath('div[@class=\"pic\"]/a/@href').extract()\n",
    "            items.append(item)\n",
    "            yield item\n",
    "        next_page = response.xpath('//span[@class=\"next\"]/a/@href')\n",
    "        if next_page:\n",
    "            url = response.urljoin(next_page[0].extract())\n",
    "            yield scrapy.Request(url, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "定义管道线路\n",
    "pipeline:\n",
    "\n",
    "    \n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "#大部分数据库api实现只有阻塞式接口\n",
    "#twisted.enterprise.adbapi提供非阻塞接口，可以访问各种关系数据库\n",
    "\n",
    "from twisted.enterprise import adbapi\n",
    "import pymysql\n",
    "import pymysql.cursors\n",
    "\n",
    "\n",
    "class MySQLPipeline(object):\n",
    "    def __init__(self):\n",
    "        dbargs = dict(\n",
    "            host = '127.0.0.1',\n",
    "            db = 'douban',\n",
    "            user = 'root',\n",
    "            passwd = '12345678',\n",
    "            cursorclass = pymysql.cursors.DictCursor,\n",
    "            charset = 'utf8',\n",
    "            use_unicode = True\n",
    "        )\n",
    "        \n",
    "        self.dbpool = adbapi.ConnectionPool('pymysql',**dbargs)\n",
    "\n",
    "    #pipeline调用\n",
    "    def process_item(self, item,spider):\n",
    "        res = self.dbpool.runInteraction(self.insert_into_table,item)\n",
    "        return item\n",
    "    \n",
    "    \n",
    "    def insert_into_table(self,conn,item):\n",
    "        conn.execute('insert into douban(rank, title, link) values(%s,%s,%s)', (\n",
    "            item['rank'][0],\n",
    "            item['title'][0],\n",
    "            item['link'][0])\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "作业：二手车之家\n",
    "\n",
    "http://www.che168.com/beijing/list/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中间件的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.\n",
    "动态网页\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "在Settings.py文件里加入\n",
    "\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'scrapy.contrib.downloadermiddlewares.useragent.UserAgentMiddleware': None,     #禁用内置中间件\n",
    "    '[爬虫名字].middlewares.AJAXMiddleware': 500,  #key为中间件类的路径，value为中间件的顺序\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "爬取下拉加载类网页："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "\n",
    "from selenium import webdriver\n",
    "from scrapy.http import HtmlResponse\n",
    "import time\n",
    "\n",
    "class AJAXMiddleware(object):\n",
    "    def process_request(self, request, spider):\n",
    "        if spider.name == \"spider_name\":\n",
    "            body = \"\"\n",
    "            print(\"PhantomJS is starting...\")\n",
    "            driver = webdriver.PhantomJS() #用PhantomJS访问\n",
    "            driver.get(request.url)\n",
    "            body = body + driver.page_source\n",
    "            for i in range(1,10):\n",
    "                time.sleep(1)\n",
    "                driver.execute_script('window.scrollTo(0,document.body.scrollHeight)') #页面拉至最底端\n",
    "                time.sleep(5)\n",
    "                body = body + driver.page_source\n",
    "                \n",
    "            return HtmlResponse(driver.current_url, body=body, encoding='utf-8', request=request)\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取“获取更多”或者“下一页”加载类网页："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "\n",
    "from selenium import webdriver\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "class AJAXMiddleware(object):\n",
    "    def process_request(self, request, spider):\n",
    "        if spider.name == \"spider_name\":\n",
    "            body = \"\"\n",
    "            print(\"PhantomJS is starting...\")\n",
    "            driver = webdriver.PhantomJS() #用PhantomJS访问\n",
    "            driver.get(request.url)\n",
    "            body = body + driver.page_source\n",
    "            for i in range(1,10):\n",
    "                next_page = driver.find_element_by_xpath(\"//\")    # “下一页“或“加载更多”按钮\n",
    "                next_page.click()    # 点击\n",
    "                body = body + driver.page_source\n",
    "            return HtmlResponse(driver.current_url, body=body, encoding='utf-8', request=request)\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "settings： \n",
    "\n",
    "设置延时：\n",
    "\n",
    "DOWNLOAD_DELAY = 0.25 # 250 ms of delay\n",
    "\n",
    "设置并发量：\n",
    "\n",
    "CONCURRENT_ITEMS、CONCURRENT_REQUESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ref：\n",
    "\n",
    "http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "https://www.xncoding.com/2016/03/08/scrapy-01.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  二手车之家\n",
    "\n",
    "http://www.che168.com/beijing/list/\n",
    "\n",
    "//*[@id=\"viewlist_ul\"]/li[3]/a/div/div[2]/em/b\n",
    "\n",
    "//ul[@class=“fn-clear”]/li/a/div/div[2]/em/b\n",
    "\n",
    "//div[@class=\"price\"]//b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
